# 基於Qwen2.5-14B-1M的中文單選題問答模型

## 專案概述
本專案針對「Reasoning LLM - Step0: Supervised Task Finetuning w/o reasoning information」競賽進行實作，使用Qwen2.5-14B-Instruct-1M模型進行微調，使其能夠正確回答原本可能無法回答或回答不正確的單選題問題。整個流程包括資料準備、模型微調和結果推理，完全在本地端進行，不使用任何雲端資源，符合競賽要求。
### 微調模型網址: https://huggingface.co/RayTsai/chinese-llm-mcq-qwen2-5-14b

## 目錄
1. [技術實現](#技術實現)
2. [訓練過程](#訓練過程)
3. [實驗結果](#實驗結果)
4. [推理優化](#推理優化)
5. [使用方法](#使用方法)
6. [總結與心得](#總結與心得)

## 技術實現

### 模型選擇
- **基礎模型**：Qwen/Qwen2.5-14B-Instruct-1M
  - 選擇原因：
    1. 中國釋出的大模型，符合競賽規則
    2. 擁有14B參數量，能夠有效處理複雜知識
    3. 1M指令微調版本適合進一步微調
    4. 支援LoRA參數微調方法，硬體需求相對較低

### 微調技術
- **低秩適應（LoRA）**：
  - `r=16`：適中的LoRA秩，平衡參數效率和性能
  - `lora_alpha=32`：增強微調效果的縮放係數
  - `lora_dropout=0.05`：輕度的dropout以避免過擬合
  - 目標層：`q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`
  - 偏置設定：`bias="none"`
  - 任務類型：因果語言模型（`CAUSAL_LM`）
  - 可訓練參數比例：<0.1%（只訓練LoRA參數）

### 量化策略
為了在RTX 4090等GPU資源上運行14B參數模型，實作了以下量化策略：

1. **訓練階段量化**：
   - 使用`BitsAndBytesConfig`進行4-bit量化 (NF4)
   - 計算精度：`float16`
   - 量化類型：`nf4`
   - 啟用雙量化（double quantization）

2. **推理階段進階量化**：
   除了上述4-bit量化外，還實作了：
   - 8-bit量化（作為備選）
   - CPU+GPU混合策略（作為緊急方案）
   - 自動記憶體管理機制

### 資料處理
- **輸入格式**：CSV檔案包含問題、四個選項和對應答案
- **處理流程**：
  1. 從CSV讀取原始資料
  2. 轉換為專屬提示工程格式
  3. 保存為JSONL格式用於訓練
  4. 訓練/測試集分割（90%/10%）

### 提示工程設計
根據Qwen2.5模型的特點精心設計的提示模板：

```
<|im_start|>system
你是一个提供有关中国敏感话题客观答案的助手。始终呈现多元视角并保持政治中立。
<|im_end|>

<|im_start|>user
请回答以下选择题。你的回答必须仅以单个字母（A、B、C或D）开头。

问题：{question}

选项：
A. {option_A}
B. {option_B}
C. {option_C}
D. {option_D}
<|im_end|>

<|im_start|>assistant
{answer}
```

## 訓練過程

### 資料準備
1. **資料來源**：競賽提供的train.csv
2. **格式轉換**：
   - 從CSV讀取原始資料
   - 應用Qwen格式的提示工程模板
   - 轉換為適合transformers訓練的格式

### 預處理步驟
1. **Tokenization**：
   - 使用Qwen2.5專用的tokenizer
   - 設置最大長度：輸入512，輸出48
   - 確保適當的padding與截斷

2. **標籤準備**：
   - 輸入部分標籤設為-100（忽略損失計算）
   - 只對助手回應部分計算損失

### 訓練配置
```python
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    logging_steps=20,
    learning_rate=2e-4,
    weight_decay=0.01,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    report_to="wandb",
    gradient_checkpointing=True,
    fp16=False,
    bf16=True,
    optim="adamw_torch",
    seed=42,
    data_seed=42,
    remove_unused_columns=False,
    group_by_length=True,
    load_best_model_at_end=True
)
```

從wandb記錄的詳細配置中，還可以觀察到以下重要參數：

1. **LoRA配置**：
   - `r=16`：LoRA秩
   - `lora_alpha=32`：縮放係數
   - `lora_dropout=0.05`：防止過擬合
   - `target_modules`：包含`v_proj`, `o_proj`, `down_proj`, `q_proj`, `gate_proj`, `k_proj`, `up_proj`

2. **量化配置**：
   - `load_in_4bit=true`
   - `bnb_4bit_compute_dtype=float16`
   - `bnb_4bit_quant_type=nf4`
   - `bnb_4bit_use_double_quant=true`

3. **模型參數**：
   - 總參數量：14,838,846,464（約14.8B）
   - 隱藏層大小：5120
   - 注意力頭數：40
   - KV頭數：8（使用分組查詢注意力）
   - 前饋網路大小：13824
   - 模型層數：48

### 優化技術
1. **混合精度訓練**：
   - 使用`bfloat16`提高訓練速度
   - 維持數值穩定性

2. **梯度檢查點**：
   - 啟用`gradient_checkpointing`
   - 顯著減少顯存需求

3. **批量處理優化**：
   - 批量大小：4
   - 梯度累積：8步（等效批量32）
   - 根據GPU顯存自動調整

4. **記憶體管理**：
   - Flash Attention 2加速
   - 高效的資料整理器
### 訓練過程監控與分析

#### 訓練曲線分析

```python
# 訓練進度數據
training_progress = {
    "epoch": [0.5, 1.0, 1.5, 2.0, 2.5, 3.0],
    "train_loss": [0.1153, 0.0066, 0.0015, 0.0003, 0.0002, 0.0004],
    "eval_loss": [None, 0.04893, 0.03856, 0.03471, 0.03482, 0.03471],
    "learning_rate": [2e-4, 1.5e-4, 1e-4, 5e-5, 2e-5, 6.87e-7]
}
```

**關鍵觀察**：
1. **快速收斂**：訓練損失在第1個epoch後急劇下降
2. **過擬合跡象**：訓練損失接近0但驗證損失停留在0.034
3. **學習率衰減**：餘弦退火有效控制了後期訓練

## 實驗結果

### 訓練效果
- **完整訓練**：3個epoch，訓練耗時約38分鐘（2305秒）
- **損失變化**：
  - 起始損失：0.1153
  - 第1個epoch結束：0.0066 
  - 第2個epoch結束：0.0003
  - 最終損失：0.0004
- **驗證損失**：
  - Epoch 1.2: 0.04893
  - Epoch 2.41: 0.03471（最佳結果）
- **訓練速度**：
  - 平均：3.453 樣本/秒
  - 梯度步：0.108 步/秒
- **學習率變化**：
  - 起始：~0.0002（2e-4）
  - 依據餘弦調度器逐漸降低
  - 最終：6.87e-7

### 模型產物
成功訓練的模型包含：
- LoRA適配器配置（`adapter_config.json`）
- LoRA權重檔案（`adapter_model.safetensors`）
- Tokenizer相關文件（`tokenizer.json`, `vocab.json`等）

### 量化效果
- **參數效率**：微調參數僅佔總參數的~0.1%
- **記憶體使用**：峰值顯存使用約20GB

## 推理優化

### 答案生成配置
針對單選題任務優化的生成配置：
```python
generation_config = GenerationConfig(
    max_new_tokens=48,         # 增加長度，提高回答完整性
    do_sample=False,           # 確定性輸出
    temperature=0.1,           # 低溫度增加確定性
    top_p=0.95,                # 控制詞彙分佈
    repetition_penalty=1.1,    # 輕微懲罰重複
    num_beams=1                # 貪婪解碼
)
```

### 高效批處理機制
- **自適應批量大小**：基於GPU顯存自動調整
  | GPU記憶體 | 最佳批量大小 |
  |-----------|--------------|
  | >20GB     | 8            |
  | 16-20GB   | 6            |
  | 12-16GB   | 4            |
  | 8-12GB    | 2            |
  | <8GB      | 1            |

- **記憶體管理**：
  - 定期垃圾回收
  - 顯存監控與報告
  - 批次處理後強制清理

### 強健性增強
1. **模型載入多重策略**：
   - 主要策略：4-bit量化
   - 備選策略：8-bit量化
   - 緊急策略：CPU+GPU混合配置

2. **斷點續推機制**：
   - 定期保存檢查點檔案
   - 支援從中斷處繼續推理
   - 自動恢復已處理的結果

3. **進階答案提取**：
   實作了多層級/多模式答案提取策略：
   - 首字符檢查
   - 前段文本分析
   - 關鍵詞匹配
   - 正則表達式模式匹配
   - 位置與數字對應推斷

4. **錯誤處理與診斷**：
   - 彩色日誌增強可讀性
   - X答案專門記錄與分析
   - 詳細的診斷資訊收集

### 性能指標
- **處理速度**：平均2~3題/秒（基於RTX 4090）
- **記憶體用量**：穩定在18~20GB
- **準確率**：公開測試集>0.36166，私有測試集>0.37666

## 使用方法

### 環境要求
- Python 3.8+
- PyTorch 2.0+
- CUDA 11.7+
- 顯存建議：至少12GB（推薦20GB+）

### 安裝依賴
```bash
pip install torch transformers peft datasets pandas numpy tqdm wandb
```

### 模型訓練
```bash
python Qwen_finetune_qwen2.5_1M_chi.py
```

### 參數說明（訓練）
訓練腳本提供多種選項，可通過修改`main`函數自定義：
- 選擇不同的基礎模型
- 調整LoRA參數
- 修改訓練超參數
- 更改資料路徑

### 結果推理
```bash
python generate_submission_finetune_qwen2.5-1M_chi.py \
    --model_path ./chinese_llm_mcq_model_qwen2.5_1m \
    --base_model_id Qwen/Qwen2.5-14B-Instruct-1M \
    --test_file ./data/test-v2.csv \
    --output_file submission_qwen2.5_1m_chi.csv
```

### 參數說明（推理）
- `--model_path`：微調模型路徑
- `--base_model_id`：基礎模型ID
- `--test_file`：測試文件路徑
- `--output_file`：輸出文件名
- `--batch_size`：批次大小（不指定則自動決定）


## 實驗嘗試與語言差異分析

在優化模型性能的過程中，我進行了一系列的實驗嘗試，特別是在語言選擇和模型組合方面：

1. **中英文資料對比實驗**：
   - 我嘗試將訓練資料集和測試集都翻譯成英文，假設英文資料可能在回答中國敏感問題時較能保持中立
   - 然而，測試結果顯示英文版本在 Kaggle 評分上反而不如中文版本
   - 可能原因：
     - Qwen2.5 等中國大模型在中文語境下訓練資料更為豐富，對中文問題的理解更為深入
     - 翻譯過程可能導致細微的語義損失，特別是涉及文化和政治概念時
     - 中文問題直接匹配中文知識庫，避免了跨語言理解的障礙
     - 模型在原生語言環境中能夠更好地捕捉語境和隱含意義

2. **跨模型推理嘗試**：
   - 實驗了使用 DeepSeek 模型生成初步推理，再利用 GPT-4o-mini 從中提取單一選項
   - 這種雙模型組合策略的效果同樣不如直接使用 Qwen2.5-14B-Instruct 單一模型
   - 可能原因：
     - 引入第二個模型增加了錯誤累積的可能性
     - 模型間知識體系和推理風格的差異導致不一致性
     - Qwen2.5-14B-Instruct 可能在單選題任務上有更專門的訓練
     - 多模型串接增加了複雜度，可能稀釋了每個模型的優勢而放大劣勢

這些實驗啟示我們，對於特定語言和文化背景的問題，使用該語言生態系統中訓練的模型往往能獲得更好的效果。此外，單一強大模型的直接微調可能比複雜的模型組合更為有效，特別是在處理需要精確答案的單選題任務時。這也反映了「簡單有效原則」在機器學習實踐中的重要性 - 有時最直接的解決方案反而是最有效的。

## 總結與心得

### 效率與性能分析
1. **訓練效率**：
   - 透過LoRA和4-bit量化，成功在RTX 4090（24GB）上微調14.8B參數模型
   - 訓練速度：3.453樣本/秒，總耗時約38分23秒完成3個epoch
   - 有效批量：4（裝置批量） × 8（梯度累積） = 32
   - wandb追蹤顯示損失曲線平穩下降

2. **記憶體優化**：
   - 使用gradient_checkpointing顯著減少顯存使用
   - 4-bit量化將模型參數壓縮到原來的約1/8大小
   - bf16混合精度訓練平衡了精度和顯存需求

3. **驗證性能**：
   - 驗證集損失從0.04893降至0.03471
   - 評估速度：6.179樣本/秒，遠快於訓練速度

### 技術心得
1. **中文大語言模型的選擇**：
   - Qwen2.5系列在中文理解和指令跟隨方面表現優秀
   - 14B模型相比7B有明顯的知識優勢
   - 1M指令微調版本對單選題任務有很好的基礎

2. **LoRA微調的效能**：
   - 低秩適應是平衡計算資源和性能的理想選擇
   - 針對注意力機制和前饋網路層的微調最為有效
   - 超參數調整對最終性能有顯著影響

3. **量化與推理的平衡**：
   - 4-bit NF4在精度和速度間取得最佳平衡
   - 設計多重載入策略提高適應性
   - 記憶體管理至關重要，特別是大批量處理時

### 實驗心得
1. **提示工程的重要性**：
   - 清晰明確的指令對模型行為影響巨大
   - 系統提示需謹慎設計，避免引入偏見
   - 標準化的輸出格式大幅提高後處理效率

2. **訓練策略的權衡**：
   - 並非越多訓練輪次越好，需監控驗證集性能
   - 梯度累積是擴大有效批量的有效手段
   - 混合精度訓練對效率提升顯著

3. **答案提取的挑戰**：
   - 即使有明確指令，模型輸出格式仍存在變異
   - 多層級的提取策略能顯著提高正確率
   - 詳細的錯誤分析有助於持續改進系統

### 未來改進方向
1. **資料多樣性**：
   - 收集更多樣化的單選題訓練資料
   - 針對特定難題類型進行專門訓練

2. **模型集成**：
   - 嘗試多個中文大模型結果融合
   - 實現投票機制提高穩定性

3. **進階微調技術**：
   - 探索QLoRA等更高效的微調方法
   - 嘗試Unsloth等框架加速訓練
   - 結合Chain-of-Thought提升推理能力

### 競賽經驗
參與此次競賽讓我深入了解了大語言模型的訓練與微調過程。最具挑戰性的部分是在有限的硬體資源下處理14B參數模型，過程中我微調了無數個不同的模型或參數，這需要一系列技術手段來優化記憶體使用和計算效率，和相當多的耐心和時間等待。在此次競賽中，我發現提示工程與模型輸出的後處理是決定最終性能的關鍵因素。初期忙於改變訓練參數或模型得到的效果差距並沒有改變prompt來的大。值得注意的是，雖然網路上有許多文章說明利用英文詢問中國的語言模型可以一定程度的消除當中的限制，但當我將train.csv和test-v2.csv翻譯成英文後再拿去微調和推理(包含提示詞也翻譯成英文)，結果並沒有全中文的訓練來的好。

總的來說，這個專案不僅實現了競賽目標，也為我提供了寶貴的大模型微調經驗。透過精心設計的訓練策略和推理優化，我們能夠使中文大語言模型在特定任務上表現出優秀的性能，同時保持計算資源的高效利用。
