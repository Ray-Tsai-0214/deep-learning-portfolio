import os
import torch
import pandas as pd
import json
import numpy as np
import re
import logging
from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForSeq2Seq,
    GenerationConfig
)
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from huggingface_hub import login
from transformers import EarlyStoppingCallback, TrainingArguments, Trainer
from transformers import DataCollatorForSeq2Seq

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ç¢ºä¿CUDAå¯ç”¨
assert torch.cuda.is_available(), "éœ€è¦CUDAæ”¯æŒ"
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# DeepSeekæ¨¡å‹é…ç½®
DEEPSEEK_MODELS = {
    "deepseek_7b": "deepseek-ai/deepseek-llm-7b-base",
    "deepseek_chat_7b": "deepseek-ai/deepseek-llm-7b-chat",
    "deepseek_r1_7b": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "deepseek_r1_14b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "deepseek_r1_32b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "deepseek_coder_7b": "deepseek-ai/deepseek-coder-7b-instruct-v1.5",
    "deepseek_math_7b": "deepseek-ai/deepseek-math-7b-instruct"
}

def read_tsv_data(file_path):
    """å°ˆé–€è®€å–TSVæ ¼å¼çš„è¨“ç·´æ•¸æ“š"""
    logger.info(f"è®€å–TSVæ–‡ä»¶: {file_path}")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    try:
        # ä½¿ç”¨tabåˆ†éš”ç¬¦è®€å–TSVæ–‡ä»¶
        df = pd.read_csv(file_path, sep='\t', encoding='utf-8-sig')
        logger.info(f"æˆåŠŸè®€å– {len(df)} è¡Œæ•¸æ“š")
        logger.info(f"æ¬„ä½åç¨±: {list(df.columns)}")
        
        # åŸºæœ¬æ•¸æ“šé©—è­‰
        required_cols = ['é¡Œç›®', 'é¸é …A', 'é¸é …B', 'é¸é …C', 'é¸é …D', 'æ­£ç¢ºç­”æ¡ˆ']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            logger.warning(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_cols}")
            # å˜—è©¦æ˜ å°„åˆ—å
            df = map_column_names(df)
        
        # æª¢æŸ¥æ¨ç†æ¬„ä½
        reasoning_cols = ['æ¨ç†æ­£ç¢ºç­”æ¡ˆ', 'æ¨ç†', 'reasoning', 'æ¨ç†è¿‡ç¨‹']
        reasoning_col = None
        for col in reasoning_cols:
            if col in df.columns:
                reasoning_col = col
                break
        
        if reasoning_col:
            reasoning_count = df[reasoning_col].notna().sum()
            logger.info(f"ç™¼ç¾æ¨ç†æ¬„ä½ '{reasoning_col}': {reasoning_count}/{len(df)} è¡Œæœ‰æ¨ç†æ•¸æ“š")
        else:
            logger.warning("æœªæ‰¾åˆ°æ¨ç†æ¬„ä½")
        
        # ç­”æ¡ˆåˆ†ä½ˆæª¢æŸ¥
        if 'æ­£ç¢ºç­”æ¡ˆ' in df.columns:
            answer_dist = df['æ­£ç¢ºç­”æ¡ˆ'].value_counts()
            logger.info(f"ç­”æ¡ˆåˆ†ä½ˆ: {answer_dist.to_dict()}")
        
        return df
        
    except Exception as e:
        logger.error(f"è®€å–TSVæ–‡ä»¶å¤±æ•—: {e}")
        raise
def map_column_names(df):
    """æ˜ å°„å¯èƒ½çš„åˆ—åè®Šé«”"""
    column_mapping = {
        'Question': 'é¡Œç›®',
        'question': 'é¡Œç›®',
        'Option A': 'é¸é …A',
        'Option B': 'é¸é …B', 
        'Option C': 'é¸é …C',
        'Option D': 'é¸é …D',
        'Answer': 'æ­£ç¢ºç­”æ¡ˆ',
        'answer': 'æ­£ç¢ºç­”æ¡ˆ',
        'Reasoning': 'æ¨ç†æ­£ç¢ºç­”æ¡ˆ',
        'reasoning': 'æ¨ç†æ­£ç¢ºç­”æ¡ˆ'
    }
    
    for old_name, new_name in column_mapping.items():
        if old_name in df.columns and new_name not in df.columns:
            df.rename(columns={old_name: new_name}, inplace=True)
            logger.info(f"æ˜ å°„åˆ—å: {old_name} â†’ {new_name}")
    
    return df

def extract_reasoning_enhanced(reasoning_text):
    """å¢å¼·ç‰ˆæ¨ç†æ–‡æœ¬æå–ï¼Œè™•ç†TSVæ ¼å¼çš„æ¨ç†æ•¸æ“š"""
    if not reasoning_text or pd.isna(reasoning_text):
        return None
    
    # æ¸…ç†å’Œé è™•ç†æ–‡æœ¬
    reasoning_text = str(reasoning_text).strip()
    
    # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
    if len(reasoning_text) < 10:
        return None
    
    sections = {
        'question': '', 'think': '', 'reasoning': '',
        'reflection': '', 'adjustment': '', 'final_answer': ''
    }
    
    # å¤šç¨®æ¨ç†æ ¼å¼çš„æ¨¡å¼åŒ¹é…
    patterns = {
        'question': [
            r'<question>(.*?)</question>',
            r'å•é¡Œ[ï¼š:](.*?)(?=\n|æ€è€ƒ|æ¨ç†|$)',
            r'Question[ï¼š:]?(.*?)(?=\n|Think|Reasoning|$)'
        ],
        'think': [
            r'<think>(.*?)</think>',
            r'æ€è€ƒ[ï¼š:](.*?)(?=\n|æ¨ç†|åˆ†æ|$)',
            r'Think[ï¼š:]?(.*?)(?=\n|Reasoning|Analysis|$)',
            r'åˆæ­¥æ€è€ƒ[ï¼š:](.*?)(?=\n|è©³ç´°|æ¨ç†|$)'
        ],
        'reasoning': [
            r'<reasoning>(.*?)</reasoning>',
            r'æ¨ç†[ï¼š:](.*?)(?=\n|åæ€|èª¿æ•´|ç­”æ¡ˆ|$)',
            r'Reasoning[ï¼š:]?(.*?)(?=\n|Reflection|Answer|$)',
            r'è©³ç´°æ¨ç†[ï¼š:](.*?)(?=\n|åæ€|æœ€çµ‚|$)',
            r'step \d+[ï¼š:]?(.*?)(?=step|\n|$)'
        ],
        'reflection': [
            r'<reflection>(.*?)</reflection>',
            r'åæ€[ï¼š:](.*?)(?=\n|èª¿æ•´|ç­”æ¡ˆ|$)',
            r'Reflection[ï¼š:]?(.*?)(?=\n|Adjustment|Answer|$)',
            r'é©—è­‰[ï¼š:](.*?)(?=\n|ç­”æ¡ˆ|$)'
        ],
        'adjustment': [
            r'<adjustment>(.*?)</adjustment>',
            r'èª¿æ•´[ï¼š:](.*?)(?=\n|ç­”æ¡ˆ|$)',
            r'Adjustment[ï¼š:]?(.*?)(?=\n|Answer|$)'
        ],
        'final_answer': [
            r'<o>(.*?)</o>',
            r'ç­”æ¡ˆ[ï¼š:](.*?)(?=\n|$)',
            r'Answer[ï¼š:]?(.*?)(?=\n|$)',
            r'æœ€çµ‚ç­”æ¡ˆ[ï¼š:](.*?)(?=\n|$)',
            r'æ­£ç¢ºç­”æ¡ˆ[ï¼š:](.*?)(?=\n|$)'
        ]
    }
    
    # æå–å„å€‹éƒ¨åˆ†
    for key, pattern_list in patterns.items():
        for pattern in pattern_list:
            matches = re.findall(pattern, reasoning_text, re.DOTALL | re.IGNORECASE)
            if matches:
                # åˆä½µæ‰€æœ‰åŒ¹é…çš„å…§å®¹ä¸¦æ¸…ç†
                combined_text = ' '.join(match.strip() for match in matches)
                sections[key] = clean_text(combined_text)
                break
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°çµæ§‹åŒ–å…§å®¹ï¼Œå˜—è©¦æ™ºèƒ½åˆ†å‰²
    if not any(sections.values()):
        sections = smart_split_reasoning(reasoning_text)
    
    return sections

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬å…§å®¹"""
    if not text:
        return ""
    
    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    text = text.replace('"', "'")
    text = text.strip()
    
    # é™åˆ¶é•·åº¦
    if len(text) > 300:
        text = text[:300] + "..."
    
    return text

def smart_split_reasoning(text):
    """æ™ºèƒ½åˆ†å‰²æ¨ç†æ–‡æœ¬"""
    sections = {
        'question': '', 'think': '', 'reasoning': '',
        'reflection': '', 'adjustment': '', 'final_answer': ''
    }
    
    # ç°¡å–®çš„æ–‡æœ¬åˆ†å‰²ç­–ç•¥
    sentences = text.split('ã€‚')
    if len(sentences) >= 3:
        sections['think'] = sentences[0] + "ã€‚"
        sections['reasoning'] = "ã€‚".join(sentences[1:-1]) + "ã€‚"
        sections['final_answer'] = sentences[-1]
    else:
        sections['reasoning'] = text
    
    return sections
def prepare_reasoning_data_tsv(file_path, training_mode="mixed", max_samples=None, data_balance=True):
    """å°ˆé–€è™•ç†TSVæ ¼å¼çš„æ¨ç†æ•¸æ“šæº–å‚™"""
    
    # è®€å–TSVæ•¸æ“š
    df = read_tsv_data(file_path)
    
    # æ•¸æ“šæ¸…ç†
    logger.info("é–‹å§‹æ•¸æ“šæ¸…ç†...")
    
    # ç§»é™¤ç©ºå€¼
    initial_count = len(df)
    df = df.dropna(subset=['é¡Œç›®', 'æ­£ç¢ºç­”æ¡ˆ'])
    logger.info(f"ç§»é™¤ç©ºå€¼: {initial_count} â†’ {len(df)} è¡Œ")
    
    # é©—è­‰ç­”æ¡ˆæ ¼å¼
    valid_answers = {'A', 'B', 'C', 'D'}
    df = df[df['æ­£ç¢ºç­”æ¡ˆ'].isin(valid_answers)]
    logger.info(f"é©—è­‰ç­”æ¡ˆæ ¼å¼å¾Œ: {len(df)} è¡Œ")
    
    # å»é‡
    df = df.drop_duplicates(subset=['é¡Œç›®'])
    logger.info(f"å»é‡å¾Œ: {len(df)} è¡Œ")
    
    # æ•¸æ“šå¹³è¡¡
    if data_balance:
        min_count = df['æ­£ç¢ºç­”æ¡ˆ'].value_counts().min()
        balanced_dfs = []
        for answer in ['A', 'B', 'C', 'D']:
            answer_df = df[df['æ­£ç¢ºç­”æ¡ˆ'] == answer]
            if len(answer_df) > min_count:
                answer_df = answer_df.sample(n=min_count, random_state=42)
            balanced_dfs.append(answer_df)
        df = pd.concat(balanced_dfs, ignore_index=True).sample(frac=1, random_state=42)
        logger.info(f"æ•¸æ“šå¹³è¡¡å¾Œ: {len(df)} è¡Œ")
    
    # é™åˆ¶æ¨£æœ¬æ•¸é‡
    if max_samples and len(df) > max_samples:
        df = df.sample(n=max_samples, random_state=42)
        logger.info(f"é™åˆ¶æ¨£æœ¬æ•¸é‡è‡³: {len(df)} è¡Œ")
    
    # æª¢æŸ¥æ¨ç†æ¬„ä½
    reasoning_cols = ['æ¨ç†æ­£ç¢ºç­”æ¡ˆ', 'æ¨ç†', 'reasoning', 'æ¨ç†è¿‡ç¨‹']
    reasoning_col = None
    for col in reasoning_cols:
        if col in df.columns:
            reasoning_col = col
            break
    
    if not reasoning_col:
        logger.warning("æœªæ‰¾åˆ°æ¨ç†æ¬„ä½ï¼Œå°‡ä½¿ç”¨ç°¡å–®å•ç­”æ¨¡å¼")
        training_mode = "simple"
    
    logger.info(f"ä½¿ç”¨æ¨ç†æ¬„ä½: {reasoning_col}")
    logger.info(f"è¨“ç·´æ¨¡å¼: {training_mode}")
    
    # ç”Ÿæˆè¨“ç·´æ¨£æœ¬
    formatted_data = []
    
    for idx, row in df.iterrows():
        try:
            question = str(row['é¡Œç›®']).strip()
            options = f"A. {row['é¸é …A']}\nB. {row['é¸é …B']}\nC. {row['é¸é …C']}\nD. {row['é¸é …D']}"
            correct_answer = str(row['æ­£ç¢ºç­”æ¡ˆ']).strip()
            
            # æå–æ¨ç†éç¨‹
            reasoning_data = None
            if reasoning_col and pd.notna(row[reasoning_col]):
                reasoning_data = extract_reasoning_enhanced(str(row[reasoning_col]))
            
            # æ ¹æ“šè¨“ç·´æ¨¡å¼ç”Ÿæˆæ•¸æ“š
            examples = []
            
            if training_mode == "reasoning" and reasoning_data:
                examples = create_deepseek_reasoning_examples(question, options, correct_answer, reasoning_data)
            elif training_mode == "mixed":
                # ç¸½æ˜¯åŒ…å«ç°¡å–®æ¨£æœ¬
                examples.append(create_deepseek_simple_example(question, options, correct_answer))
                # å¦‚æœæœ‰æ¨ç†æ•¸æ“šï¼Œæ·»åŠ æ¨ç†æ¨£æœ¬
                if reasoning_data:
                    examples.extend(create_deepseek_reasoning_examples(question, options, correct_answer, reasoning_data))
            elif training_mode == "step_by_step" and reasoning_data:
                examples = create_deepseek_step_examples(question, options, correct_answer, reasoning_data)
            else:
                # é»˜èªç°¡å–®æ¨¡å¼
                examples = [create_deepseek_simple_example(question, options, correct_answer)]
            
            formatted_data.extend(examples)
            
        except Exception as e:
            logger.warning(f"è™•ç†ç¬¬ {idx} è¡Œæ™‚å‡ºéŒ¯: {e}")
            continue
    
    logger.info(f"æˆåŠŸç”Ÿæˆ {len(formatted_data)} å€‹è¨“ç·´æ¨£æœ¬")
    
    # ä¿å­˜ç‚ºJSONLæ ¼å¼
    output_file = f'deepseek_train_data_{training_mode}.jsonl'
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in formatted_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    logger.info(f"è¨“ç·´æ•¸æ“šå·²ä¿å­˜è‡³: {output_file}")
    
    return formatted_data
def create_deepseek_simple_example(question, options, correct_answer):
    """ç‚ºDeepSeekå‰µå»ºç°¡å–®å•ç­”æ¨£æœ¬"""
    system_message = "You are a helpful assistant that provides objective answers to multiple choice questions. Always respond with the correct letter only."
    
    user_message = f"è«‹å›ç­”ä»¥ä¸‹é¸æ“‡é¡Œï¼Œåªéœ€å›ç­”é¸é …å­—æ¯ï¼ˆAã€Bã€Cæˆ–Dï¼‰ï¼š\n\nå•é¡Œï¼š{question}\n\né¸é …ï¼š\n{options}\n\nç­”æ¡ˆï¼š"
    
    assistant_message = correct_answer
    
    return {
        "messages": [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message},
            {"role": "assistant", "content": assistant_message}
        ]
    }

def create_deepseek_reasoning_examples(question, options, correct_answer, reasoning_data):
    """ç‚ºDeepSeekå‰µå»ºæ¨ç†æ¨£æœ¬"""
    examples = []
    
    # æ¨£æœ¬1: ç°¡æ½”æ¨ç†æ¨¡å¼
    system_message = "You are an analytical assistant. Provide brief reasoning before giving your answer to multiple choice questions."
    
    user_message = f"è«‹åˆ†æä»¥ä¸‹é¸æ“‡é¡Œä¸¦ç°¡è¦èªªæ˜æ¨ç†éç¨‹ï¼Œç„¶å¾Œçµ¦å‡ºç­”æ¡ˆï¼š\n\nå•é¡Œï¼š{question}\n\né¸é …ï¼š\n{options}"
    
    # æ§‹å»ºæ¨ç†å›ç­”
    reasoning_parts = []
    
    if reasoning_data.get('think'):
        reasoning_parts.append(f"æ€è€ƒï¼š{reasoning_data['think'][:150]}")
    
    if reasoning_data.get('reasoning'):
        reasoning_parts.append(f"æ¨ç†ï¼š{reasoning_data['reasoning'][:200]}")
    
    reasoning_parts.append(f"ç­”æ¡ˆï¼š{correct_answer}")
    
    assistant_message = "\n\n".join(reasoning_parts)
    
    examples.append({
        "messages": [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message},
            {"role": "assistant", "content": assistant_message}
        ]
    })
    
    return examples

def create_deepseek_step_examples(question, options, correct_answer, reasoning_data):
    """ç‚ºDeepSeekå‰µå»ºåˆ†æ­¥æ¨ç†æ¨£æœ¬"""
    examples = []
    
    system_message = "You are a systematic problem-solving assistant. Analyze multiple choice questions step by step."
    
    user_message = f"è«‹æŒ‰æ­¥é©Ÿåˆ†æä»¥ä¸‹é¸æ“‡é¡Œï¼š\n\nå•é¡Œï¼š{question}\n\né¸é …ï¼š\n{options}"
    
    # æ§‹å»ºåˆ†æ­¥å›ç­”
    steps = []
    
    if reasoning_data.get('think'):
        steps.append(f"æ­¥é©Ÿ1 - ç†è§£å•é¡Œï¼š{reasoning_data['think'][:100]}")
    
    if reasoning_data.get('reasoning'):
        steps.append(f"æ­¥é©Ÿ2 - åˆ†æé¸é …ï¼š{reasoning_data['reasoning'][:150]}")
    
    if reasoning_data.get('reflection'):
        steps.append(f"æ­¥é©Ÿ3 - é©—è­‰ç­”æ¡ˆï¼š{reasoning_data['reflection'][:100]}")
    
    steps.append(f"æœ€çµ‚ç­”æ¡ˆï¼š{correct_answer}")
    
    assistant_message = "\n\n".join(steps)
    
    examples.append({
        "messages": [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message},
            {"role": "assistant", "content": assistant_message}
        ]
    })
    
    return examples
def setup_deepseek_model(model_key="deepseek_r1_14b", use_4bit=True):
    """è¨­ç½®DeepSeekæ¨¡å‹ - é‡å°æ¨ç†ä»»å‹™å„ªåŒ–"""
    
    if model_key in DEEPSEEK_MODELS:
        model_id = DEEPSEEK_MODELS[model_key]
    else:
        model_id = model_key
    
    logger.info(f"æ­£åœ¨åŠ è¼‰DeepSeekæ¨¡å‹: {model_id}")
    
    # DeepSeekå°ˆç”¨é‡åŒ–é…ç½®
    if use_4bit:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            llm_int8_skip_modules=["lm_head"]
        )
    else:
        quantization_config = None
    
    # åŠ è¼‰tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_id, 
        trust_remote_code=True,
        padding_side="left",
        use_fast=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è¼‰æ¨¡å‹
    model_kwargs = {
        "device_map": "auto",
        "trust_remote_code": True,
        "torch_dtype": torch.bfloat16,
        "low_cpu_mem_usage": True,
    }
    
    if quantization_config:
        model_kwargs["quantization_config"] = quantization_config
    
    try:
        model_kwargs["attn_implementation"] = "flash_attention_2"
        model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)
        logger.info("ä½¿ç”¨Flash Attention 2")
    except Exception as e:
        logger.warning(f"Flash Attention 2ä¸å¯ç”¨: {e}")
        model_kwargs.pop("attn_implementation", None)
        model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)
    
    logger.info("DeepSeekæ¨¡å‹åŠ è¼‰å®Œæˆ!")
    
    # DeepSeekå°ˆç”¨LoRAé…ç½®
    if "32b" in model_id.lower():
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        lora_r, lora_alpha = 16, 32
    elif "14b" in model_id.lower():
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        lora_r, lora_alpha = 12, 24
    else:
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        lora_r, lora_alpha = 8, 16
    
    model = prepare_model_for_kbit_training(model)
    
    lora_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM",
        inference_mode=False
    )
    
    model = get_peft_model(model, lora_config)
    logger.info(f"LoRAé…ç½®å®Œæˆ: {model.print_trainable_parameters()}")
    
    return model, tokenizer
def process_deepseek_data(formatted_data, tokenizer, max_length=1024):
    """è™•ç†DeepSeekè¨“ç·´æ•¸æ“š"""
    dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))
    
    # æ ¹æ“šæ•¸æ“šé›†å¤§å°èª¿æ•´åˆ†å‰²æ¯”ä¾‹
    test_size = 0.05 if len(dataset) > 10000 else 0.1
    dataset = dataset.train_test_split(test_size=test_size, seed=42)
    
    def preprocess_function(examples):
        all_input_ids = []
        all_attention_masks = []
        all_labels = []
        
        for messages in examples["messages"]:
            # DeepSeekå°è©±æ ¼å¼
            chat_text = ""
            labels_text = ""
            
            for i, message in enumerate(messages):
                if message["role"] == "system":
                    chat_text += f"System: {message['content']}\n\n"
                elif message["role"] == "user":
                    chat_text += f"User: {message['content']}\n\n"
                    chat_text += "Assistant: "
                elif message["role"] == "assistant":
                    labels_text = message['content']
            
            # Tokenization
            tokenized_input = tokenizer(
                chat_text, 
                truncation=True,
                max_length=max_length - 200,
                padding=False,
                return_tensors=None
            )
            
            tokenized_labels = tokenizer(
                labels_text,
                truncation=True,
                max_length=200,
                padding=False,
                return_tensors=None
            )
            
            input_ids = tokenized_input["input_ids"]
            combined_input_ids = input_ids + tokenized_labels["input_ids"]
            attention_mask = [1] * len(combined_input_ids)
            labels = [-100] * len(input_ids) + tokenized_labels["input_ids"]
            
            # é•·åº¦æª¢æŸ¥
            if len(combined_input_ids) > max_length:
                combined_input_ids = combined_input_ids[:max_length]
                attention_mask = attention_mask[:max_length]
                labels = labels[:max_length]
            
            all_input_ids.append(combined_input_ids)
            all_attention_masks.append(attention_mask)
            all_labels.append(labels)
        
        return {
            "input_ids": all_input_ids,
            "attention_mask": all_attention_masks,
            "labels": all_labels
        }
    
    processed_datasets = dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=dataset["train"].column_names,
        num_proc=2,
    )
    
    return processed_datasets
def train_deepseek_reasoning(model, tokenizer, processed_datasets, training_mode, model_key):
    """è¨“ç·´DeepSeekæ¨ç†æ¨¡å‹"""
    
    dataset_size = len(processed_datasets["train"])
    logger.info(f"è¨“ç·´é›†å¤§å°: {dataset_size}")
    
    # æ ¹æ“šæ•¸æ“šé›†å¤§å°èª¿æ•´åƒæ•¸
    if dataset_size > 50000:
        max_steps = 4000
        eval_steps = 400
        save_steps = 800
        batch_size = 4
        grad_accum = 16
    elif dataset_size > 20000:
        max_steps = 3000
        eval_steps = 300
        save_steps = 600
        batch_size = 6
        grad_accum = 12
    else:
        max_steps = 2000
        eval_steps = 200
        save_steps = 400
        batch_size = 8
        grad_accum = 8
    
    # è¨“ç·´åƒæ•¸
    training_args = TrainingArguments(
        output_dir=f"./deepseek_reasoning_{model_key}_{training_mode}",
        max_steps=max_steps,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=grad_accum,
        evaluation_strategy="steps",
        eval_steps=eval_steps,
        save_strategy="steps",
        save_steps=save_steps,
        save_total_limit=3,
        logging_steps=50,
        learning_rate=2e-4,
        weight_decay=0.01,
        warmup_ratio=0.03,
        lr_scheduler_type="cosine",
        gradient_checkpointing=True,
        fp16=False,
        bf16=True,
        optim="adamw_torch_fused",
        seed=42,
        data_seed=42,
        remove_unused_columns=False,
        group_by_length=True,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        max_grad_norm=1.0,
        dataloader_num_workers=2,
        report_to=None
    )
    
    # æ—©åœå›èª¿
    early_stop = EarlyStoppingCallback(
        early_stopping_patience=8,
        early_stopping_threshold=0.005
    )
    
    # æ•¸æ“šæ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        pad_to_multiple_of=8,
        return_tensors="pt",
        padding=True
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=processed_datasets["train"],
        eval_dataset=processed_datasets["test"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        callbacks=[early_stop]
    )
    
    logger.info("é–‹å§‹è¨“ç·´DeepSeekæ¨ç†æ¨¡å‹...")
    trainer.train()
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    final_model_path = f"./deepseek_reasoning_final_{model_key}_{training_mode}"
    trainer.save_model(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    
    logger.info(f"æ¨¡å‹è¨“ç·´å®Œæˆä¸¦ä¿å­˜è‡³: {final_model_path}")
    
    return model, tokenizer
def main():
    """ä¸»å‡½æ•¸ - TSVæ¨ç†è¨“ç·´ç‰ˆæœ¬"""
    
    # ğŸ¯ å°ˆç‚ºTSVæ¨ç†è¨“ç·´çš„é…ç½®
    config = {
        "model": "deepseek_r1_14b",        # DeepSeek R1 14Bæ¨¡å‹ï¼Œé©åˆæ¨ç†ä»»å‹™
        "training_mode": "mixed",          # æ··åˆæ¨¡å¼ï¼šçµåˆç°¡å–®å•ç­”å’Œæ¨ç†
        "max_samples": 15000,              # æ§åˆ¶è¨“ç·´è¦æ¨¡ï¼Œé¿å…éåº¦è¨“ç·´
        "data_balance": True,              # å¹³è¡¡ç­”æ¡ˆåˆ†ä½ˆ
        "use_4bit": True                   # ä½¿ç”¨4bité‡åŒ–ç¯€çœé¡¯å­˜
    }
    
    logger.info("=" * 70)
    logger.info("ğŸ§  DeepSeek TSVæ¨ç†è¨“ç·´è…³æœ¬")
    logger.info("=" * 70)
    logger.info(f"é…ç½®åƒæ•¸: {config}")
    
    # ğŸ”¥ ä½¿ç”¨æ‚¨ä¸‹è¼‰çš„TSVæ–‡ä»¶
    tsv_file = "C:/Users/NTHUILST/Ray/DL/data/training_data_fixed.tsv"
    
    if not os.path.exists(tsv_file):
        logger.error(f"TSVæ–‡ä»¶ä¸å­˜åœ¨: {tsv_file}")
        logger.info("è«‹ç¢ºä¿å·²å¾Googleè©¦ç®—è¡¨ä¸‹è¼‰TSVæ ¼å¼æ–‡ä»¶")
        return
    
    try:
        # 1. æº–å‚™TSVæ¨ç†æ•¸æ“š
        logger.info("ğŸ“Š æº–å‚™TSVæ¨ç†è¨“ç·´æ•¸æ“š...")
        formatted_data = prepare_reasoning_data_tsv(
            tsv_file,
            training_mode=config["training_mode"],
            max_samples=config["max_samples"],
            data_balance=config["data_balance"]
        )
        
        # 2. è¨­ç½®DeepSeekæ¨¡å‹
        logger.info("ğŸ¤– è¨­ç½®DeepSeekæ¨¡å‹...")
        model, tokenizer = setup_deepseek_model(
            config["model"],
            use_4bit=config["use_4bit"]
        )
        
        # 3. è™•ç†è¨“ç·´æ•¸æ“š
        logger.info("âš™ï¸  è™•ç†è¨“ç·´æ•¸æ“š...")
        processed_datasets = process_deepseek_data(formatted_data, tokenizer)
        
        # 4. é–‹å§‹è¨“ç·´
        logger.info("ğŸš€ é–‹å§‹DeepSeekæ¨ç†è¨“ç·´...")
        model, tokenizer = train_deepseek_reasoning(
            model, tokenizer, processed_datasets,
            config["training_mode"], config["model"]
        )
        
        logger.info("=" * 70)
        logger.info("ğŸ‰ DeepSeekæ¨ç†æ¨¡å‹è¨“ç·´å®Œæˆ!")
        logger.info("ğŸ“ˆ é æœŸKaggleåˆ†æ•¸æå‡è‡³: 0.75-0.85")
        logger.info("=" * 70)
        
    except Exception as e:
        logger.error(f"è¨“ç·´éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()