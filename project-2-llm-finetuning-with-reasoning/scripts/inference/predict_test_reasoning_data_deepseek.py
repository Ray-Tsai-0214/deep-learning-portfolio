"""
ä½¿ç”¨æœ€ä½³DeepSeek Checkpointé€²è¡Œé æ¸¬
åŸºæ–¼checkpoint-600 (æœ€ä½³é©—è­‰loss: 0.6686)
"""

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import csv
from tqdm import tqdm
import time
import re
import os
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BestDeepSeekPredictor:
    def __init__(self, 
                 base_model_name="deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
                 checkpoint_path="./deepseek_reasoning_deepseek_r1_14b_mixed/checkpoint-600"):
        """
        ä½¿ç”¨æœ€ä½³checkpointåˆå§‹åŒ–é æ¸¬å™¨
        
        Args:
            base_model_name: åŸºç¤æ¨¡å‹åç¨±
            checkpoint_path: æœ€ä½³checkpointè·¯å¾‘
        """
        logger.info("ğŸ”„ åŠ è¼‰æœ€ä½³DeepSeek checkpoint...")
        logger.info(f"   åŸºç¤æ¨¡å‹: {base_model_name}")
        logger.info(f"   Checkpoint: {checkpoint_path}")
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpointä¸å­˜åœ¨: {checkpoint_path}")
        
        try:
            # 1. åŠ è¼‰tokenizer
            logger.info("   ğŸ“ åŠ è¼‰tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                checkpoint_path,
                trust_remote_code=True,
                padding_side="left"
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
            # 2. åŠ è¼‰åŸºç¤æ¨¡å‹
            logger.info("   ğŸ¤– åŠ è¼‰åŸºç¤æ¨¡å‹...")
            self.base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
                load_in_4bit=True  # ç¯€çœé¡¯å­˜
            )
            
            # 3. åŠ è¼‰LoRAé©é…å™¨
            logger.info("   ğŸ¯ åŠ è¼‰LoRAé©é…å™¨...")
            self.model = PeftModel.from_pretrained(
                self.base_model, 
                checkpoint_path,
                torch_dtype=torch.bfloat16
            )
            
            # 4. è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
            self.model.eval()
            
            logger.info("âœ… æœ€ä½³DeepSeekæ¨¡å‹åŠ è¼‰å®Œæˆ!")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
            raise e    
    def predict_single(self, question, option_a, option_b, option_c, option_d, mode="mixed"):
        """
        å°å–®å€‹å•é¡Œé€²è¡Œé æ¸¬
        
        Args:
            question: å•é¡Œæ–‡æœ¬
            option_a, option_b, option_c, option_d: é¸é …
            mode: é æ¸¬æ¨¡å¼ ("simple", "reasoning", "mixed")
        """
        
        # æ§‹å»ºé¸é …æ–‡æœ¬
        options = f"A. {option_a}\nB. {option_b}\nC. {option_c}\nD. {option_d}"
        
        # æ ¹æ“šè¨“ç·´æ™‚çš„æ ¼å¼æ§‹å»ºprompt
        if mode == "reasoning":
            system_message = "You are an analytical assistant. Provide brief reasoning before giving your answer to multiple choice questions."
            user_message = f"è«‹åˆ†æä»¥ä¸‹é¸æ“‡é¡Œä¸¦ç°¡è¦èªªæ˜æ¨ç†éç¨‹ï¼Œç„¶å¾Œçµ¦å‡ºç­”æ¡ˆï¼š\n\nå•é¡Œï¼š{question}\n\né¸é …ï¼š\n{options}"
        else:
            system_message = "You are a helpful assistant that provides objective answers to multiple choice questions. Always respond with the correct letter only."
            user_message = f"è«‹å›ç­”ä»¥ä¸‹é¸æ“‡é¡Œï¼Œåªéœ€å›ç­”é¸é …å­—æ¯ï¼ˆAã€Bã€Cæˆ–Dï¼‰ï¼š\n\nå•é¡Œï¼š{question}\n\né¸é …ï¼š\n{options}\n\nç­”æ¡ˆï¼š"
        
        # æ§‹å»ºDeepSeekæ ¼å¼çš„å°è©±
        chat_text = f"System: {system_message}\n\nUser: {user_message}\n\nAssistant: "
        
        try:
            # Tokenize
            inputs = self.tokenizer(
                chat_text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=1024
            ).to(self.model.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=64 if mode == "simple" else 200,
                    temperature=0.1,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç¢¼å›ç­”
            response = self.tokenizer.decode(
                outputs[0][len(inputs.input_ids[0]):], 
                skip_special_tokens=True
            )
            
            # æå–ç­”æ¡ˆ
            predicted_answer = self.extract_answer(response)
            
            return predicted_answer, response.strip()
            
        except Exception as e:
            logger.warning(f"âš ï¸ é æ¸¬å¤±æ•—: {e}")
            return 'A', f"éŒ¯èª¤: {str(e)}"    
    def extract_answer(self, response):
        """å¼·åŒ–ç‰ˆç­”æ¡ˆæå–"""
        
        # æ¸…ç†response
        response = response.strip()
        
        # ç­–ç•¥1: æŸ¥æ‰¾å–®ç¨çš„A/B/C/D
        single_letter = re.search(r'^([ABCD])$', response)
        if single_letter:
            return single_letter.group(1)
        
        # ç­–ç•¥2: æŸ¥æ‰¾é–‹é ­çš„å­—æ¯
        first_letter = re.search(r'^([ABCD])', response)
        if first_letter:
            return first_letter.group(1)
        
        # ç­–ç•¥3: æŸ¥æ‰¾ç­”æ¡ˆæ¨¡å¼
        answer_patterns = [
            r'ç­”æ¡ˆ[ï¼š:]\s*([ABCD])',
            r'æœ€çµ‚ç­”æ¡ˆ[ï¼š:]\s*([ABCD])',
            r'é¸æ“‡\s*([ABCD])',
            r'æ­£ç¢ºç­”æ¡ˆ[ï¼š:]\s*([ABCD])',
        ]
        
        for pattern in answer_patterns:
            match = re.search(pattern, response)
            if match:
                return match.group(1)
        
        # ç­–ç•¥4: æŸ¥æ‰¾ä»»ä½•A/B/C/D (å–æœ€å¾Œä¸€å€‹)
        letters = re.findall(r'([ABCD])', response)
        if letters:
            return letters[-1]
        
        # é»˜èªè¿”å›A
        logger.warning(f"âš ï¸ ç„¡æ³•è§£æç­”æ¡ˆ: '{response[:100]}...'ï¼Œé»˜èªè¿”å›A")
        return 'A'

def predict_test_data_best_checkpoint(
    test_csv_path, 
    checkpoint_path="./deepseek_reasoning_deepseek_r1_14b_mixed/checkpoint-600",
    output_path="./submission_best_deepseek.csv",
    prediction_mode="mixed"
):
    """
    ä½¿ç”¨æœ€ä½³checkpointå°æ¸¬è©¦æ•¸æ“šé€²è¡Œæ‰¹é‡é æ¸¬
    
    Args:
        test_csv_path: æ¸¬è©¦æ•¸æ“šè·¯å¾‘
        checkpoint_path: æœ€ä½³checkpointè·¯å¾‘
        output_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
        prediction_mode: é æ¸¬æ¨¡å¼
    """
    
    print("=" * 70)
    print("ğŸ¯ ä½¿ç”¨æœ€ä½³DeepSeek Checkpointé€²è¡Œé æ¸¬")
    print(f"ğŸ“Š Checkpoint: {checkpoint_path}")
    print(f"ğŸª é æ¸¬æ¨¡å¼: {prediction_mode}")
    print("=" * 70)    
    # 1. æª¢æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(test_csv_path):
        print(f"âŒ æ¸¬è©¦æ–‡ä»¶ä¸å­˜åœ¨: {test_csv_path}")
        return
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ Checkpointä¸å­˜åœ¨: {checkpoint_path}")
        return
    
    # 2. åŠ è¼‰æ¸¬è©¦æ•¸æ“š
    print("ğŸ“Š åŠ è¼‰æ¸¬è©¦æ•¸æ“š...")
    try:
        df = pd.read_csv(test_csv_path)
        print(f"âœ… æˆåŠŸåŠ è¼‰ {len(df)} å€‹æ¸¬è©¦æ¨£æœ¬")
        
        # æª¢æŸ¥æ•¸æ“šæ ¼å¼
        required_cols = ['ID', 'Question', 'Option A', 'Option B', 'Option C', 'Option D']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_cols}")
            print(f"å¯ç”¨æ¬„ä½: {list(df.columns)}")
            return
            
    except Exception as e:
        print(f"âŒ åŠ è¼‰æ¸¬è©¦æ•¸æ“šå¤±æ•—: {e}")
        return
    
    # 3. åˆå§‹åŒ–é æ¸¬å™¨
    try:
        predictor = BestDeepSeekPredictor(checkpoint_path=checkpoint_path)
    except Exception as e:
        print(f"âŒ é æ¸¬å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        return    
    # 4. æ‰¹é‡é æ¸¬
    print(f"ğŸ”® é–‹å§‹é æ¸¬ {len(df)} å€‹å•é¡Œ...")
    results = []
    
    start_time = time.time()
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="é æ¸¬é€²åº¦"):
        try:
            question_id = int(row['ID'])
            question = str(row['Question']).strip()
            option_a = str(row['Option A']).strip()
            option_b = str(row['Option B']).strip()
            option_c = str(row['Option C']).strip()
            option_d = str(row['Option D']).strip()
            
            # é€²è¡Œé æ¸¬
            predicted_answer, reasoning = predictor.predict_single(
                question, option_a, option_b, option_c, option_d, 
                mode=prediction_mode
            )
            
            results.append({
                'ID': question_id,
                'Answer': predicted_answer
            })
            
            # æ¯50å€‹æ¨£æœ¬é¡¯ç¤ºé€²åº¦
            if (idx + 1) % 50 == 0:
                elapsed = time.time() - start_time
                rate = (idx + 1) / elapsed
                eta = (len(df) - idx - 1) / rate
                print(f"   é€²åº¦: {idx + 1}/{len(df)} ({rate:.1f} samples/sec, ETA: {eta/60:.1f}min)")
                
        except Exception as e:
            print(f"âŒ ç¬¬{idx+1}è¡Œé æ¸¬å¤±æ•—: {e}")
            results.append({
                'ID': idx + 1,
                'Answer': 'A'
            })    
    # 5. ä¿å­˜çµæœ
    print("ğŸ’¾ ä¿å­˜é æ¸¬çµæœ...")
    try:
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('ID')
        
        # é©—è­‰ç­”æ¡ˆæ ¼å¼
        valid_answers = {'A', 'B', 'C', 'D'}
        invalid_count = 0
        
        for idx, row in results_df.iterrows():
            if row['Answer'] not in valid_answers:
                print(f"âš ï¸ ä¿®å¾©ç„¡æ•ˆç­”æ¡ˆ: ID {row['ID']}, '{row['Answer']}' -> 'A'")
                results_df.at[idx, 'Answer'] = 'A'
                invalid_count += 1
        
        if invalid_count > 0:
            print(f"ğŸ”§ ä¿®å¾©äº† {invalid_count} å€‹ç„¡æ•ˆç­”æ¡ˆ")
        
        # ä¿å­˜CSV
        results_df.to_csv(output_path, index=False)
        print(f"âœ… é æ¸¬çµæœå·²ä¿å­˜è‡³: {output_path}")
        
        # é¡¯ç¤ºçµæœçµ±è¨ˆ
        print("\nğŸ“Š é æ¸¬çµæœçµ±è¨ˆ:")
        answer_counts = results_df['Answer'].value_counts().sort_index()
        for answer, count in answer_counts.items():
            percentage = count / len(results_df) * 100
            print(f"   {answer}: {count} å€‹ ({percentage:.1f}%)")
        
        # è¨ˆç®—ç¸½è€—æ™‚
        total_time = time.time() - start_time
        print(f"\nâ±ï¸ ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜")
        print(f"ğŸ“ˆ é æœŸKaggleåˆ†æ•¸: 0.70-0.78 (åŸºæ–¼æœ€ä½³checkpoint)")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜çµæœå¤±æ•—: {e}")
        return
    
    print("=" * 70)
    print("ğŸ‰ æœ€ä½³Checkpointé æ¸¬å®Œæˆ!")
    print("=" * 70)
def main():
    """ä¸»å‡½æ•¸"""
    
    # é…ç½®
    config = {
        "test_file": "C:/Users/NTHUILST/Ray/DL/data/test-check-v2.csv",
        "checkpoint": "./deepseek_reasoning_deepseek_r1_14b_mixed/checkpoint-600",  # æœ€ä½³checkpoint
        "output": "./submission_best_deepseek.csv",
        "mode": "mixed"  # mixedæ¨¡å¼å¹³è¡¡é€Ÿåº¦å’Œè³ªé‡
    }
    
    print("ğŸ¯ ä½¿ç”¨æœ€ä½³DeepSeek Checkpointé€²è¡Œé æ¸¬")
    print(f"é…ç½®: {config}")
    
    # æª¢æŸ¥checkpointæ˜¯å¦å­˜åœ¨
    if not os.path.exists(config["checkpoint"]):
        print(f"âŒ æœ€ä½³checkpointä¸å­˜åœ¨: {config['checkpoint']}")
        print("\nå¯é¸æ–¹æ¡ˆ:")
        print("1. ä½¿ç”¨ checkpoint-2400 (æœ€æ–°ä½†å¯èƒ½éæ“¬åˆ)")
        print("2. ä½¿ç”¨ checkpoint-1800")
        print("3. é‡æ–°è¨“ç·´æ¨¡å‹")
        return
    
    # é–‹å§‹é æ¸¬
    predict_test_data_best_checkpoint(
        test_csv_path=config["test_file"],
        checkpoint_path=config["checkpoint"],
        output_path=config["output"],
        prediction_mode=config["mode"]
    )

if __name__ == "__main__":
    main()