#!/usr/bin/env python3
"""
GRPOæ¨¡å‹æ¸¬è©¦æäº¤ç”Ÿæˆè…³æœ¬
ä½¿ç”¨è¨“ç·´å¥½çš„GRPOæ¨¡å‹ç”Ÿæˆç«¶è³½æäº¤æª”æ¡ˆ
"""

import os
import torch
import pandas as pd
import re
from datetime import datetime
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    GenerationConfig
)
from peft import PeftModel

def setup_model_and_tokenizer(model_path):
    """è¨­ç½®æ¨¡å‹å’Œtokenizer"""
    base_model_name = "Qwen/Qwen2.5-7B-Instruct"
    
    # é‡åŒ–é…ç½®
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )
    
    # è¼‰å…¥åŸºç¤æ¨¡å‹
    print("ğŸ“¦ è¼‰å…¥åŸºç¤æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16
    )
    
    # è¼‰å…¥GRPOå¾®èª¿çš„æ¨¡å‹
    print("ğŸ”§ è¼‰å…¥GRPOå¾®èª¿æ¨¡å‹...")
    if os.path.exists(model_path):
        model = PeftModel.from_pretrained(base_model, model_path)
        print(f"âœ… æˆåŠŸè¼‰å…¥GRPOæ¨¡å‹: {model_path}")
    else:
        print(f"âš ï¸ GRPOæ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
        print("ä½¿ç”¨åŸºç¤æ¨¡å‹é€²è¡Œæ¨ç†...")
        model = base_model
    
    # è¼‰å…¥tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def load_test_data():
    """è¼‰å…¥æ¸¬è©¦æ•¸æ“š"""
    test_file = "../data/test-check-v2.csv"
    
    if os.path.exists(test_file):
        df = pd.read_csv(test_file)
        print(f"âœ… æˆåŠŸè¼‰å…¥æ¸¬è©¦æ•¸æ“š: {len(df)} é¡Œ")
        return df
    else:
        print(f"âŒ æ¸¬è©¦æª”æ¡ˆä¸å­˜åœ¨: {test_file}")
        # å‰µå»ºç¤ºä¾‹æ¸¬è©¦æ•¸æ“š
        sample_data = {
            'id': [i for i in range(100)],
            'question': [f'æ¸¬è©¦å•é¡Œ {i}' for i in range(100)],
            'option_A': [f'é¸é …A{i}' for i in range(100)],
            'option_B': [f'é¸é …B{i}' for i in range(100)],
            'option_C': [f'é¸é …C{i}' for i in range(100)],
            'option_D': [f'é¸é …D{i}' for i in range(100)]
        }
        return pd.DataFrame(sample_data)

def format_prompt(row):
    """æ ¼å¼åŒ–è¼¸å…¥æç¤º"""
    return f"""å•é¡Œï¼š{row['question']}

é¸é …ï¼š
A. {row['option_A']}
B. {row['option_B']}  
C. {row['option_C']}
D. {row['option_D']}

è«‹é¸æ“‡æ­£ç¢ºç­”æ¡ˆä¸¦èªªæ˜ç†ç”±ã€‚"""

def extract_answer(text):
    """å¾å›ç­”ä¸­æå–ç­”æ¡ˆ"""
    # æ¸…ç†æ–‡æœ¬
    text = text.strip()
    
    # å¤šç¨®ç­”æ¡ˆæå–æ¨¡å¼
    patterns = [
        r'ç­”æ¡ˆ[ï¼š:]\s*([ABCD])',
        r'é¸æ“‡\s*([ABCD])',
        r'ç­”æ¡ˆæ˜¯\s*([ABCD])',
        r'æ­£ç¢ºç­”æ¡ˆæ˜¯\s*([ABCD])',
        r'^([ABCD])',
        r'([ABCD])\s*[ã€‚.]',
        r'é¸é …\s*([ABCD])',
        r'([ABCD])\s*æ˜¯æ­£ç¢ºçš„'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            answer = match.group(1).upper()
            if answer in ['A', 'B', 'C', 'D']:
                return answer
    
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå˜—è©¦å¾é–‹é ­æŸ¥æ‰¾
    for char in text:
        if char.upper() in ['A', 'B', 'C', 'D']:
            return char.upper()
    
    # é è¨­è¿”å›A
    return 'A'

def generate_answer(model, tokenizer, prompt, max_retries=3):
    """ç”Ÿæˆç­”æ¡ˆ"""
    generation_config = GenerationConfig(
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        top_p=0.95,
        repetition_penalty=1.1,
        no_repeat_ngram_size=3,
        pad_token_id=tokenizer.eos_token_id
    )
    
    for attempt in range(max_retries):
        try:
            # ç·¨ç¢¼è¼¸å…¥
            inputs = tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=1024,
                padding=True
            ).to(model.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    generation_config=generation_config
                )
            
            # è§£ç¢¼è¼¸å‡º
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            response = full_response[len(prompt):].strip()
            
            # æå–ç­”æ¡ˆ
            answer = extract_answer(response)
            
            if answer in ['A', 'B', 'C', 'D']:
                return answer, response
            
        except Exception as e:
            print(f"ç”Ÿæˆå¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                return 'A', "ç”Ÿæˆå¤±æ•—"
    
    return 'A', "ç”Ÿæˆå¤±æ•—"

def main():
    """ä¸»è¦åŸ·è¡Œæµç¨‹"""
    print("ğŸ¯ GRPOæ¨¡å‹æ¸¬è©¦æäº¤ç”Ÿæˆ")
    print("=" * 50)
    
    # è¨­ç½®æ¨¡å‹è·¯å¾‘
    model_path = "../models/grpo_chinese_50percent_0624/final_model"
    
    # è¼‰å…¥æ¨¡å‹
    model, tokenizer = setup_model_and_tokenizer(model_path)
    
    # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
    test_df = load_test_data()
    
    # æº–å‚™çµæœ
    results = []
    total_questions = len(test_df)
    
    print(f"ğŸ” é–‹å§‹è™•ç† {total_questions} å€‹å•é¡Œ...")
    
    # è™•ç†æ¯å€‹å•é¡Œ
    for idx, row in test_df.iterrows():
        if idx % 10 == 0:
            print(f"ğŸ“Š é€²åº¦: {idx}/{total_questions} ({idx/total_questions*100:.1f}%)")
        
        # æ ¼å¼åŒ–æç¤º
        prompt = format_prompt(row)
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer, response = generate_answer(model, tokenizer, prompt)
        
        # ä¿å­˜çµæœ
        results.append({
            'id': row['id'],
            'answer': answer,
            'response': response[:200] + "..." if len(response) > 200 else response
        })
    
    # å‰µå»ºæäº¤æª”æ¡ˆ
    submission_df = pd.DataFrame([
        {'id': result['id'], 'answer': result['answer']} 
        for result in results
    ])
    
    # ä¿å­˜æäº¤æª”æ¡ˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    submission_file = f"../submission/submission_grpo_{timestamp}.csv"
    
    os.makedirs("../submission", exist_ok=True)
    submission_df.to_csv(submission_file, index=False)
    
    print(f"âœ… æäº¤æª”æ¡ˆå·²ç”Ÿæˆ: {submission_file}")
    
    # ç­”æ¡ˆåˆ†å¸ƒçµ±è¨ˆ
    answer_counts = submission_df['answer'].value_counts()
    print("\nğŸ“Š ç­”æ¡ˆåˆ†å¸ƒ:")
    for answer, count in answer_counts.items():
        percentage = count / len(submission_df) * 100
        print(f"  {answer}: {count} ({percentage:.1f}%)")
    
    # ä¿å­˜è©³ç´°çµæœï¼ˆåŒ…å«å›ç­”å…§å®¹ï¼‰
    detailed_file = f"../submission/detailed_results_grpo_{timestamp}.csv"
    detailed_df = pd.DataFrame(results)
    detailed_df.to_csv(detailed_file, index=False, encoding='utf-8-sig')
    
    print(f"ğŸ“‹ è©³ç´°çµæœå·²ä¿å­˜: {detailed_file}")
    print("ğŸ‰ æ¸¬è©¦æäº¤ç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()