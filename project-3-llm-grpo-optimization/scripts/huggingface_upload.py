#!/usr/bin/env python3
"""
HuggingFaceæ¨¡å‹ä¸Šå‚³è…³æœ¬
å°‡è¨“ç·´å®Œæˆçš„GRPOæ¨¡å‹ä¸Šå‚³åˆ°HuggingFace Hub
"""

import os
import json
from datetime import datetime
from huggingface_hub import HfApi, login, create_repo, upload_folder

# HuggingFace token (è«‹è¨­ç½®ç’°å¢ƒè®Šæ•¸æˆ–ç›´æ¥å¡«å…¥)
TOKEN = os.getenv("HUGGINGFACE_TOKEN", "your_token_here")

def create_model_card():
    """å‰µå»ºæ¨¡å‹å¡ç‰‡"""
    return """---
language:
- zh
license: apache-2.0
tags:
- chinese
- llm
- lora
- qwen2.5
- grpo
- reinforcement-learning
base_model: Qwen/Qwen2.5-7B-Instruct
datasets:
- custom-chinese-reasoning
model-index:
- name: chinese-grpo-qwen2.5-7b-50percent
  results:
    - task:
        type: text-generation
      metrics:
        - name: Reward Score
          type: reward
          value: 0.66
        - name: Training Loss
          type: loss
          value: 0.058
---

# Chinese GRPO Qwen2.5-7B (50% Dataset)

ä½¿ç”¨GRPO (Group Relative Policy Optimization)æ–¹æ³•è¨“ç·´çš„ä¸­æ–‡å¤§èªè¨€æ¨¡å‹ï¼Œå°ˆé–€å„ªåŒ–è™•ç†æ•æ„Ÿè­°é¡Œçš„ä¸­ç«‹æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚

## æ¨¡å‹è©³æƒ…

- **åŸºç¤æ¨¡å‹**: Qwen/Qwen2.5-7B-Instruct
- **è¨“ç·´æ–¹æ³•**: GRPO with LoRA
- **è¨“ç·´æ•¸æ“š**: 50%ä¸­æ–‡æ¨ç†æ•¸æ“šé›† (12,238å€‹preference pairs)
- **è¨“ç·´æ™‚é–“**: 39å°æ™‚44åˆ†é˜
- **ç¡¬é«”**: NVIDIA RTX 4090 24GB
- **æœ€çµ‚çå‹µåˆ†æ•¸**: 0.66

## è¨“ç·´é…ç½®

```yaml
LoRAé…ç½®:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

è¨“ç·´åƒæ•¸:
  learning_rate: 3e-05
  batch_size: 16
  gradient_accumulation_steps: 2
  num_epochs: 2
  total_steps: 5,506
  
å„ªåŒ–è¨­ç½®:
  quantization: 4-bit
  gradient_checkpointing: true
  bf16: true
  dataloader_num_workers: 0  # è§£æ±ºpickleéŒ¯èª¤
```

## æ€§èƒ½æŒ‡æ¨™

- **æœ€çµ‚è¨“ç·´æå¤±**: 0.058
- **çå‹µåˆ†æ•¸**: 0.6604 Â± 0.068
- **KLæ•£åº¦**: 1.90
- **è™•ç†tokens**: 65,428,674

## ä½¿ç”¨æ–¹æ³•

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import torch

# é‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

# è¼‰å…¥åŸºç¤æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)

# è¼‰å…¥LoRAæ¬Šé‡
model = PeftModel.from_pretrained(
    base_model,
    "RayTsai/chinese-grpo-qwen2.5-7b-50percent"
)

# è¼‰å…¥tokenizer
tokenizer = AutoTokenizer.from_pretrained("RayTsai/chinese-grpo-qwen2.5-7b-50percent")

# ä½¿ç”¨æ¨¡å‹
prompt = "å•é¡Œï¼š[æ‚¨çš„å•é¡Œ]\\n\\né¸é …ï¼š\\nA. [é¸é …A]\\nB. [é¸é …B]\\nC. [é¸é …C]\\nD. [é¸é …D]\\n\\nè«‹é¸æ“‡æ­£ç¢ºç­”æ¡ˆä¸¦èªªæ˜ç†ç”±ã€‚"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## ç‰¹è‰²åŠŸèƒ½

1. **ä¸­ç«‹æ€§å„ªåŒ–**: é€šéGRPOè¨“ç·´æå‡å›ç­”çš„å®¢è§€æ€§
2. **æ¨ç†èƒ½åŠ›**: ä¸åƒ…çµ¦å‡ºç­”æ¡ˆï¼Œé‚„æä¾›è©³ç´°æ¨ç†éç¨‹
3. **ç©©å®šæ€§**: 40å°æ™‚è¨“ç·´ç¢ºä¿æ¨¡å‹æ”¶æ–‚
4. **æ•ˆç‡**: 4-bité‡åŒ–æ”¯æ´åœ¨æ¶ˆè²»ç´šGPUé‹è¡Œ

## è¨“ç·´æ—¥èªŒ

- é–‹å§‹æ™‚é–“: 2024-06-24 22:31:39
- çµæŸæ™‚é–“: 2024-06-26 14:19:35
- ç¸½æ­¥æ•¸: 5,506/5,508 (99.96%)
- ä¿å­˜æª¢æŸ¥é»: 27å€‹ï¼ˆæ¯200æ­¥ï¼‰

## æŠ€è¡“å‰µæ–°

1. **GRPOåœ¨ä¸­æ–‡æ•æ„Ÿè­°é¡Œçš„é¦–æ¬¡æ‡‰ç”¨**
2. **ä¸­ç«‹æ€§æ„ŸçŸ¥çš„çå‹µå‡½æ•¸è¨­è¨ˆ**
3. **è§£æ±ºPickleéŒ¯èª¤çš„å·¥ç¨‹å„ªåŒ–**
4. **é•·æ™‚é–“ç©©å®šè¨“ç·´çš„å¯¦ç¾**

## å¼•ç”¨

```bibtex
@misc{chinese-grpo-2025,
  author = {Ray Tsai},
  title = {Chinese GRPO Qwen2.5-7B 50% Dataset},
  year = {2025},
  publisher = {HuggingFace},
  url = {https://huggingface.co/RayTsai/chinese-grpo-qwen2.5-7b-50percent}
}
```

## æˆæ¬Š

Apache License 2.0

## è¯ç¹«æ–¹å¼

å¦‚æœ‰å•é¡Œæˆ–åˆä½œæ„å‘ï¼Œè«‹é€šéHuggingFaceå¹³å°è¯ç¹«ã€‚

---

**å…è²¬è²æ˜**: æ­¤æ¨¡å‹ç”¨æ–¼å­¸è¡“ç ”ç©¶ç›®çš„ï¼Œç”Ÿæˆçš„å…§å®¹ä¸ä»£è¡¨ä½œè€…è§€é»ã€‚ä½¿ç”¨æ™‚è«‹éµå®ˆç›¸é—œæ³•è¦ã€‚
"""

def upload_model():
    """ä¸Šå‚³æ¨¡å‹åˆ°HuggingFace"""
    print("ğŸš€ é–‹å§‹ä¸Šå‚³GRPOæ¨¡å‹åˆ°HuggingFace...")
    
    if TOKEN == "your_token_here":
        print("âŒ è«‹å…ˆè¨­ç½®HuggingFace token!")
        print("æ–¹æ³•1: è¨­ç½®ç’°å¢ƒè®Šæ•¸ HUGGINGFACE_TOKEN")
        print("æ–¹æ³•2: ç›´æ¥åœ¨è…³æœ¬ä¸­ä¿®æ”¹TOKENè®Šæ•¸")
        return
    
    # ç™»å…¥
    login(token=TOKEN, add_to_git_credential=False)
    api = HfApi()
    
    # æ¨¡å‹è·¯å¾‘å’Œåç¨±
    model_path = "../models/grpo_chinese_50percent_0624/final_model"
    repo_name = "chinese-grpo-qwen2.5-7b-50percent"
    repo_id = f"RayTsai/{repo_name}"
    
    print(f"ğŸ“¦ æ¨¡å‹è·¯å¾‘: {model_path}")
    print(f"ğŸ“ ç›®æ¨™repo: {repo_id}")
    
    # æª¢æŸ¥æ¨¡å‹è·¯å¾‘
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
        print("è«‹ç¢ºä¿GRPOè¨“ç·´å·²å®Œæˆä¸¦ä¿å­˜äº†æ¨¡å‹")
        return
    
    try:
        # å‰µå»ºæˆ–æ›´æ–°repo
        create_repo(repo_id=repo_id, repo_type="model", exist_ok=True)
        print("âœ… Repositoryå·²æº–å‚™å°±ç·’")
        
        # å‰µå»ºREADME
        readme_path = os.path.join(model_path, "README.md")
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(create_model_card())
        print("âœ… æ¨¡å‹å¡ç‰‡å·²å‰µå»º")
        
        # ä¸Šå‚³æ‰€æœ‰æª”æ¡ˆ
        print("ğŸ“¤ é–‹å§‹ä¸Šå‚³æª”æ¡ˆ...")
        upload_folder(
            folder_path=model_path,
            repo_id=repo_id,
            repo_type="model",
            commit_message="Upload GRPO fine-tuned model (50% Chinese dataset, 40hrs training)"
        )
        
        print("âœ… ä¸Šå‚³æˆåŠŸï¼")
        print(f"ğŸ”— æ¨¡å‹é€£çµ: https://huggingface.co/{repo_id}")
        
        # ä¿å­˜ä¸Šå‚³è¨˜éŒ„
        upload_log = {
            "upload_time": datetime.now().isoformat(),
            "repo_id": repo_id,
            "model_path": model_path,
            "status": "success"
        }
        
        log_path = "../logs/huggingface_upload.json"
        os.makedirs("../logs", exist_ok=True)
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(upload_log, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ ä¸Šå‚³è¨˜éŒ„å·²ä¿å­˜: {log_path}")
        
        # æ¸…ç†è‡¨æ™‚README
        if os.path.exists(readme_path):
            os.remove(readme_path)
            
    except Exception as e:
        print(f"âŒ ä¸Šå‚³å¤±æ•—: {e}")
        
        # ä¿å­˜éŒ¯èª¤è¨˜éŒ„
        error_log = {
            "upload_time": datetime.now().isoformat(),
            "repo_id": repo_id,
            "model_path": model_path,
            "status": "failed",
            "error": str(e)
        }
        
        log_path = "../logs/huggingface_upload_error.json"
        os.makedirs("../logs", exist_ok=True)
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(error_log, f, ensure_ascii=False, indent=2)
        
        raise

def test_connection():
    """æ¸¬è©¦HuggingFaceé€£æ¥"""
    print("ğŸ” æ¸¬è©¦HuggingFaceé€£æ¥...")
    
    if TOKEN == "your_token_here":
        print("âŒ è«‹å…ˆè¨­ç½®HuggingFace token!")
        return False
    
    try:
        login(token=TOKEN, add_to_git_credential=False)
        api = HfApi()
        user_info = api.whoami()
        
        print("âœ… é€£æ¥æˆåŠŸï¼")
        print(f"ğŸ‘¤ ç”¨æˆ¶å: {user_info.get('name', 'N/A')}")
        print(f"ğŸ“§ Email: {user_info.get('email', 'N/A')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é€£æ¥å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ¤— HuggingFace æ¨¡å‹ä¸Šå‚³å·¥å…·")
    print("=" * 50)
    
    # æ¸¬è©¦é€£æ¥
    if test_connection():
        print("\n" + "=" * 50)
        upload_model()
    else:
        print("\nâŒ è«‹æª¢æŸ¥tokenè¨­ç½®å¾Œé‡è©¦")