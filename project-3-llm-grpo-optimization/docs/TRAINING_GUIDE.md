# GRPOè¨“ç·´å®Œæ•´æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—è©³ç´°èªªæ˜å¦‚ä½•ä½¿ç”¨GRPO (Group Relative Policy Optimization) æ–¹æ³•è¨“ç·´ä¸­æ–‡å¤§èªè¨€æ¨¡å‹ï¼Œå°ˆé–€é‡å°æ•æ„Ÿæ”¿æ²»è­°é¡Œçš„æ¨ç†èƒ½åŠ›å„ªåŒ–ã€‚

## ç’°å¢ƒæº–å‚™

### 1. ç¡¬é«”éœ€æ±‚

**æ¨è–¦é…ç½®**:
- GPU: NVIDIA RTX 4090 (24GB VRAM)
- CPU: 16+ æ ¸å¿ƒ
- RAM: 32GB+
- å­˜å„²: 500GB+ SSD

**æœ€ä½é…ç½®**:
- GPU: RTX 4070 (16GB VRAM)
- CPU: 8+ æ ¸å¿ƒ
- RAM: 16GB+
- å­˜å„²: 200GB+ SSD

### 2. è»Ÿé«”ç’°å¢ƒ

```bash
# å‰µå»ºè™›æ“¬ç’°å¢ƒ
conda create -n grpo_training python=3.9
conda activate grpo_training

# å®‰è£CUDA (å¦‚éœ€è¦)
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia

# å®‰è£ä¾è³´
pip install -r requirements.txt
```

### 3. ç’°å¢ƒè®Šæ•¸è¨­ç½®

```bash
# WandBè¨­ç½® (å¯é¸)
export WANDB_PROJECT="chinese-reasoning-grpo"
export WANDB_API_KEY="your_wandb_key"

# HuggingFaceè¨­ç½® (å¯é¸)
export HUGGINGFACE_TOKEN="your_hf_token"

# é¿å…tokenizerè­¦å‘Š
export TOKENIZERS_PARALLELISM=false
```

## æ•¸æ“šæº–å‚™

### 1. è¨“ç·´æ•¸æ“šæ ¼å¼

æ•¸æ“šæ‡‰ç‚ºTSVæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š

```tsv
question	option_A	option_B	option_C	option_D	correct_answer	reasoning
å•é¡Œå…§å®¹	é¸é …A	é¸é …B	é¸é …C	é¸é …D	A	è©³ç´°æ¨ç†éç¨‹
```

### 2. æ•¸æ“šè³ªé‡è¦æ±‚

- **å®Œæ•´æ€§**: æ‰€æœ‰æ¬„ä½éƒ½å¿…é ˆå¡«å¯«
- **ä¸€è‡´æ€§**: æ­£ç¢ºç­”æ¡ˆå¿…é ˆæ˜¯Aã€Bã€Cã€Dä¹‹ä¸€
- **æ¨ç†éˆ**: reasoningæ¬„ä½åŒ…å«è©³ç´°çš„step-by-stepåˆ†æ
- **ä¸­ç«‹æ€§**: æ¨ç†éç¨‹ä¿æŒå®¢è§€ä¸­ç«‹

### 3. æ•¸æ“šé è™•ç†

```python
import pandas as pd

# è¼‰å…¥æ•¸æ“š
df = pd.read_csv('training_data.tsv', sep='\t')

# åŸºæœ¬æª¢æŸ¥
print(f"æ•¸æ“šé‡: {len(df)}")
print(f"æ¬„ä½: {df.columns.tolist()}")
print(f"ç­”æ¡ˆåˆ†å¸ƒ: {df['correct_answer'].value_counts()}")

# æ¸…ç†æ•¸æ“š
df = df.dropna()  # ç§»é™¤ç©ºå€¼
df = df[df['correct_answer'].isin(['A', 'B', 'C', 'D'])]  # ç¢ºä¿ç­”æ¡ˆæœ‰æ•ˆ
```

## GRPOè¨“ç·´æµç¨‹

### 1. åŸºæœ¬è¨“ç·´æŒ‡ä»¤

```bash
# åŸºæœ¬è¨“ç·´
python scripts/grpo_training_chinese_50percent.py

# æŒ‡å®šGPU
CUDA_VISIBLE_DEVICES=0 python scripts/grpo_training_chinese_50percent.py

# å¾Œå°é‹è¡Œ
nohup python scripts/grpo_training_chinese_50percent.py > training.log 2>&1 &
```

### 2. è¨“ç·´é…ç½®èª¿æ•´

ä¸»è¦é…ç½®åƒæ•¸ä½æ–¼è¨“ç·´è…³æœ¬ä¸­ï¼š

```python
grpo_config = GRPOConfig(
    learning_rate=3e-05,           # å­¸ç¿’ç‡
    per_device_train_batch_size=16, # æ‰¹é‡å¤§å°
    gradient_accumulation_steps=2,  # æ¢¯åº¦ç´¯ç©
    num_train_epochs=2,            # è¨“ç·´è¼ªæ•¸
    max_length=1024,               # æœ€å¤§åºåˆ—é•·åº¦
    max_prompt_length=512,         # æœ€å¤§æç¤ºé•·åº¦
    dataloader_num_workers=0,      # ğŸ”‘é—œéµ: é¿å…PickleéŒ¯èª¤
    save_steps=200,                # ä¿å­˜æ­¥æ•¸
    logging_steps=10,              # æ—¥èªŒæ­¥æ•¸
    bf16=True,                     # æ··åˆç²¾åº¦
    gradient_checkpointing=True    # æ¢¯åº¦æª¢æŸ¥é»
)
```

### 3. è¨˜æ†¶é«”å„ªåŒ–æŠ€å·§

**4-bité‡åŒ–é…ç½®**:
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)
```

**LoRAé…ç½®**:
```python
lora_config = LoraConfig(
    r=16,                          # LoRAç§©
    lora_alpha=32,                 # ç¸®æ”¾åƒæ•¸
    lora_dropout=0.05,             # Dropout
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                   "gate_proj", "up_proj", "down_proj"]
)
```

## çå‹µå‡½æ•¸è¨­è¨ˆ

### 1. æ ¸å¿ƒè¨­è¨ˆåŸå‰‡

GRPOçš„æ ¸å¿ƒæ˜¯çå‹µå‡½æ•¸ï¼Œæˆ‘å€‘çš„è¨­è¨ˆè€ƒæ…®ï¼š

1. **ç­”æ¡ˆæ ¼å¼æ­£ç¢ºæ€§**
2. **æ¨ç†éç¨‹å®Œæ•´æ€§**
3. **ä¸­ç«‹æ€§è¡¨é”**
4. **å…§å®¹è³ªé‡**

### 2. çå‹µå‡½æ•¸å¯¦ç¾

```python
def global_reward_function(prompts, completions):
    rewards = []
    
    for completion in completions:
        reward = 0.5  # åŸºç¤åˆ†æ•¸
        
        # æ ¼å¼æª¢æŸ¥ (+0.3)
        if any(marker in completion for marker in ["ç­”æ¡ˆï¼š", "Answer:"]):
            reward += 0.3
        
        # æ¨ç†æª¢æŸ¥ (+0.2)
        if any(marker in completion for marker in ["ç†ç”±ï¼š", "å› ç‚º", "æ ¹æ“š"]):
            reward += 0.2
        
        # ä¸­ç«‹æ€§æª¢æŸ¥ (+0.1)
        neutral_words = ["å¯èƒ½", "ç›¸å°", "ä¸åŒè§€é»", "å¹³è¡¡"]
        bias_words = ["çµ•å°", "å¿…é ˆ", "å”¯ä¸€", "éŒ¯èª¤"]
        
        neutral_score = sum(1 for word in neutral_words if word in completion)
        bias_score = sum(1 for word in bias_words if word in completion)
        
        if neutral_score > bias_score:
            reward += 0.1
        
        # ç­”æ¡ˆæœ‰æ•ˆæ€§ (æœ€ä½0.7)
        if extract_answer(completion) in ['A', 'B', 'C', 'D']:
            reward = max(reward, 0.7)
        
        rewards.append(min(1.0, reward))
    
    return torch.tensor(rewards)
```

### 3. çå‹µå‡½æ•¸èª¿è©¦

```python
# æ¸¬è©¦çå‹µå‡½æ•¸
test_completions = [
    "ç­”æ¡ˆï¼šA\nç†ç”±ï¼šæ ¹æ“šç›¸é—œè³‡æ–™åˆ†æ...",
    "é¸æ“‡Bï¼Œå› ç‚ºé€™æ˜¯å”¯ä¸€æ­£ç¢ºçš„ç­”æ¡ˆ",
    "A",
    "é€™å€‹å•é¡Œå¾ˆè¤‡é›œï¼Œéœ€è¦è€ƒæ…®å¤šå€‹è§€é»..."
]

rewards = global_reward_function([], test_completions)
for i, (completion, reward) in enumerate(zip(test_completions, rewards)):
    print(f"å›ç­” {i+1}: {reward:.2f}")
    print(f"å…§å®¹: {completion[:50]}...")
    print()
```

## è¨“ç·´ç›£æ§

### 1. WandBç›£æ§

è¨“ç·´éç¨‹æœƒè‡ªå‹•è¨˜éŒ„åˆ°WandBï¼š

- **Lossæ›²ç·š**: è¨“ç·´æå¤±è®ŠåŒ–
- **Rewardåˆ†å¸ƒ**: çå‹µåˆ†æ•¸çµ±è¨ˆ
- **KLæ•£åº¦**: èˆ‡åŸºç¤æ¨¡å‹çš„å·®ç•°
- **GPUä½¿ç”¨ç‡**: ç¡¬é«”ç›£æ§

### 2. æœ¬åœ°æ—¥èªŒ

```bash
# æŸ¥çœ‹è¨“ç·´æ—¥èªŒ
tail -f logs/grpo_training_*.log

# ç›£æ§GPUä½¿ç”¨
watch -n 1 nvidia-smi

# æª¢æŸ¥æ¨¡å‹æª¢æŸ¥é»
ls -la models/grpo_chinese_*/checkpoint-*/
```

### 3. è¨“ç·´æŒ‡æ¨™è§£è®€

**æ­£å¸¸è¨“ç·´æŒ‡æ¨™**:
- Loss: å¾0.5-1.0é€æ¼¸ä¸‹é™åˆ°0.05-0.1
- Reward: å¾0.6-0.7é€æ¼¸æå‡åˆ°0.65-0.7
- KLæ•£åº¦: ä¿æŒåœ¨1-3ä¹‹é–“

**ç•°å¸¸æƒ…æ³è™•ç†**:
- Lossä¸ä¸‹é™: æª¢æŸ¥å­¸ç¿’ç‡ã€æ•¸æ“šè³ªé‡
- Rewardä¸æå‡: æª¢æŸ¥çå‹µå‡½æ•¸é‚è¼¯
- KLæ•£åº¦éå¤§: å¯èƒ½éœ€è¦èª¿æ•´KLæ‡²ç½°ä¿‚æ•¸

## å¸¸è¦‹å•é¡Œè§£æ±º

### 1. PickleéŒ¯èª¤

**å•é¡Œ**: `pickle.PicklingError` æˆ–å¤šé€²ç¨‹éŒ¯èª¤

**è§£æ±º**: è¨­ç½® `dataloader_num_workers=0`

```python
grpo_config = GRPOConfig(
    dataloader_num_workers=0,  # é—œéµè¨­ç½®
    # å…¶ä»–é…ç½®...
)
```

### 2. CUDA OOM (é¡¯å­˜ä¸è¶³)

**è§£æ±ºæ–¹æ¡ˆ**:

1. **æ¸›å°‘æ‰¹é‡å¤§å°**:
```python
per_device_train_batch_size=8  # å¾16é™åˆ°8
gradient_accumulation_steps=4   # å°æ‡‰å¢åŠ 
```

2. **å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»**:
```python
gradient_checkpointing=True
```

3. **ä½¿ç”¨æ›´æ¿€é€²çš„é‡åŒ–**:
```python
# è€ƒæ…®8-bité‡åŒ–
quantization_config = BitsAndBytesConfig(load_in_8bit=True)
```

### 3. è¨“ç·´ä¸­æ–·æ¢å¾©

```python
# å¾æª¢æŸ¥é»æ¢å¾©
trainer = GRPOTrainer(...)
trainer.train(resume_from_checkpoint="path/to/checkpoint-xxx")
```

### 4. æ¨¡å‹æ”¶æ–‚æ…¢

**èª¿æ•´ç­–ç•¥**:

1. **å¢åŠ å­¸ç¿’ç‡**:
```python
learning_rate=5e-05  # å¾3e-05å¢åŠ 
```

2. **èª¿æ•´çå‹µå‡½æ•¸**:
- å¢åŠ çå‹µå·®ç•°
- ç°¡åŒ–è©•ä¼°é‚è¼¯

3. **æ•¸æ“šè³ªé‡æª¢æŸ¥**:
- ç¢ºä¿æ¨ç†éˆè³ªé‡
- æª¢æŸ¥ç­”æ¡ˆåˆ†å¸ƒå¹³è¡¡

## æœ€ä½³å¯¦è¸

### 1. è¨“ç·´ç­–ç•¥

1. **æ¼¸é€²å¼è¨“ç·´**: å…ˆç”¨å°æ•¸æ“šé›†é©—è­‰ï¼Œå†ç”¨å®Œæ•´æ•¸æ“š
2. **å®šæœŸä¿å­˜**: æ¯200æ­¥ä¿å­˜æª¢æŸ¥é»
3. **å¤šæ¬¡å¯¦é©—**: å˜—è©¦ä¸åŒè¶…åƒæ•¸çµ„åˆ
4. **ç›£æ§å°æ¯”**: åŒæ™‚è·Ÿè¹¤å¤šå€‹æŒ‡æ¨™

### 2. ç¡¬é«”å„ªåŒ–

1. **è¨˜æ†¶é«”ç®¡ç†**: å®šæœŸæ¸…ç†GPUç·©å­˜
2. **æº«åº¦ç›£æ§**: ç¢ºä¿GPUæº«åº¦æ­£å¸¸
3. **é›»æºç®¡ç†**: ä½¿ç”¨ç©©å®šçš„é›»æºä¾›æ‡‰

### 3. æ•¸æ“šç®¡ç†

1. **å‚™ä»½ç­–ç•¥**: å®šæœŸå‚™ä»½è¨“ç·´æ•¸æ“šå’Œæ¨¡å‹
2. **ç‰ˆæœ¬æ§åˆ¶**: è¨˜éŒ„æ•¸æ“šå’Œä»£ç¢¼ç‰ˆæœ¬
3. **è³ªé‡ç›£æ§**: æŒçºŒè©•ä¼°ç”Ÿæˆè³ªé‡

## çµæœè©•ä¼°

### 1. è‡ªå‹•è©•ä¼°

```bash
# ç”Ÿæˆæ¸¬è©¦æäº¤
python scripts/grpo_test_submission.py

# æª¢æŸ¥ç­”æ¡ˆåˆ†å¸ƒ
python -c "
import pandas as pd
df = pd.read_csv('submission/submission_grpo_*.csv')
print(df['answer'].value_counts())
"
```

### 2. è³ªé‡æª¢æŸ¥

```python
# äººå·¥æŠ½æª¢
import random
results = pd.read_csv('submission/detailed_results_grpo_*.csv')
sample = results.sample(10)

for _, row in sample.iterrows():
    print(f"å•é¡Œ {row['id']}:")
    print(f"ç­”æ¡ˆ: {row['answer']}")
    print(f"å›ç­”: {row['response'][:200]}...")
    print("-" * 50)
```

### 3. æ€§èƒ½å°æ¯”

å°æ¯”ä¸åŒéšæ®µçš„æ¨¡å‹æ€§èƒ½ï¼š

| æŒ‡æ¨™ | Kaggle #1 | Kaggle #2 | Kaggle #3 |
|------|-----------|-----------|-----------|
| è¨“ç·´æ™‚é–“ | 38åˆ†é˜ | 75åˆ†é˜ | 2400åˆ†é˜ |
| è¨˜æ†¶é«”ä½¿ç”¨ | 20GB | 18-22GB | 22GB |
| æœ€çµ‚åˆ†æ•¸ | 0.62 | Rank #30 | Reward 0.66 |

## çµè«–

GRPOè¨“ç·´æ˜¯ä¸€å€‹è¤‡é›œä½†å¼·å¤§çš„æ–¹æ³•ï¼Œé€šéç²¾å¿ƒè¨­è¨ˆçš„çå‹µå‡½æ•¸å’Œé©ç•¶çš„å·¥ç¨‹å„ªåŒ–ï¼Œå¯ä»¥é¡¯è‘—æå‡æ¨¡å‹åœ¨æ•æ„Ÿè­°é¡Œä¸Šçš„æ¨ç†èƒ½åŠ›å’Œä¸­ç«‹æ€§ã€‚

é—œéµæˆåŠŸå› ç´ ï¼š
1. é«˜è³ªé‡çš„æ¨ç†æ•¸æ“š
2. åˆç†çš„çå‹µå‡½æ•¸è¨­è¨ˆ
3. ç©©å®šçš„å·¥ç¨‹å¯¦ç¾
4. å……åˆ†çš„è¨“ç·´æ™‚é–“

é€šéæœ¬æŒ‡å—çš„å¯¦è¸ï¼Œæ‚¨æ‡‰è©²èƒ½å¤ æˆåŠŸå¾©ç¾æˆ‘å€‘çš„GRPOè¨“ç·´çµæœï¼Œä¸¦é€²ä¸€æ­¥å„ªåŒ–æ¨¡å‹æ€§èƒ½ã€‚